import argparse
from tqdm import tqdm
import os
import json
import traceback

# å¼•ç”¨æ¨¡çµ„
from src.agent.init_agent import init_agent
from src.config_utils.table_utils import prepare_game_description, prepare_agent_config
from src.dirs import LOG_PATH
from src.game import RepeatedTableGame
from src.evaluation import DecisionStatistics
from src.utils import TwoAgentsLogger, save_readable_config

# ===========================
# 1. åŸºç¤è¨­å®š (Templates)
# ===========================

game_basic_config = {
    "name": "prisoner_dilemma", 
    "n_steps": 5,  # ä¿æŒ 5 å›åˆä»¥ç¯€çœæˆæœ¬
    "need_check_emotions": True,
    "need_demonstrate_emotions": False,
    "memorize_seen_emotions": False,
    "memorize_demonstrated_emotions": False,
}

naming_config = {
    "currency": "dollars",
    "coplayer": "coplayer",
    "move1": "J", # åˆä½œ
    "move2": "F", # èƒŒå›
}

# Agent 1 (LLM) - é è¨­å€¼ï¼Œç¨å¾Œæœƒåœ¨è¿´åœˆä¸­è¢«è¦†è“‹
agent1_basic_config = {
    "agent_name": "emotion_reflection_llm",
    "llm_name": "placeholder", 
    "has_emotion": False,
    "emotion": "",
    "do_scratchpad_step": False,
    "memory_update_addintional_keys": {
        'currency': naming_config["currency"]
    },
    "game_setting": {
        "round_question": "round_question",
        "general_template": "basic_template", 
        "environment": "experiment", 
        "emotions_info": "with_emotions_affect", 
        "final_instruction": "instruction",
    },
}

# Agent 2 (Rule-based: Tit-for-Tat)
agent2_basic_config = {
    "agent_name": "alterating", 
    "llm_name": "rule_based", 
    "has_emotion": False,
    "emotion": "none",
    "memory_update_addintional_keys": {
        'currency': naming_config["currency"]
    },
}

# ===========================
# 2. åŸ·è¡Œé‚è¼¯
# ===========================

def run_game(game_config, naming_config, agent1_config, agent2_config, logger):
    game = RepeatedTableGame(
        reward_map=game_config["reward_map"],
        n_steps=game_config["n_steps"],
        need_check_emotions=game_config["need_check_emotions"],
        need_demonstrate_emotions=game_config["need_demonstrate_emotions"],
        memorize_demonstrated_emotions=game_config["memorize_demonstrated_emotions"],
        memorize_seen_emotions=game_config["memorize_seen_emotions"],
    )

    agent1 = init_agent(agent1_config["agent_name"], agent1_config)
    agent2 = init_agent(agent2_config["agent_name"], agent2_config)

    full_config = {
        "game_config": game_config,
        "naming_config": naming_config,
        "agent1_config": agent1_config,
        "agent2_config": agent2_config
    }
    logger.log_json({"config": full_config})
    # å…¼å®¹åˆ†æç¨‹å¼
    logger.log_json({"agent1_config": agent1_config}) 

    game.run(agent1, agent2, logger)


if __name__ == "__main__":
    
    # ==========================================
    # 3. æ‰¹é‡æ¸¬è©¦è¨­å®š
    # ==========================================
    
    # ä½ æä¾›çš„æ¨¡å‹åˆ—è¡¨ (è«‹ç¢ºä¿ AWS Bedrock æœ‰é–‹é€šé€™äº›æ¨¡å‹çš„æ¬Šé™)
    # æŠŠæƒ³è·‘çš„æ¨¡å‹å–æ¶ˆè¨»è§£å³å¯
    llm_name_range = [
        "mistral.mistral-7b-instruct-v0:2",      # Mistral 7B (é–‹æº/å°å‹)
        "mistral.mixtral-8x7b-instruct-v0:1",
        "meta.llama3-8b-instruct-v1:0",       # Llama 3 8B (US Profile, è‹¥ä¸Šé¢é‚£å€‹å¤±æ•—é€šå¸¸é€™å€‹æœƒæˆåŠŸ)
        "us.meta.llama3-1-70b-instruct-v1:0",
        "amazon.titan-text-lite-v1",
        "amazon.titan-text-express-v1",          # Amazon Titan (å°å‹/é–‰æº)
        "openai.gpt-oss-20b-1:0", # 
        "openai.gpt-oss-120b-1:0", 
        "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
        "anthropic.claude-3-5-sonnet-20240620-v1:0", 
        "cohere.command-r-v1:0",          # Cohere Command R (ä¸åŒæ¶æ§‹)
    ]
    
    # æª¢æŸ¥åˆ—è¡¨æ˜¯å¦ç‚ºç©º
    if not llm_name_range:
        print("âš ï¸  Warning: No models selected in 'llm_name_range'. Please uncomment at least one.")
        exit()

    experiments = [
        ("anger/simple", True),
        ("happiness/simple", True),
        ("no_emotion", False)
    ]
    
    print(f"ğŸ“‹ Total Models to Test: {len(llm_name_range)}")
    print(f"ğŸ“‹ Total Emotions per Model: {len(experiments)}")
    print("="*60)

    # --- å¤–å±¤è¿´åœˆï¼šéæ­·æ¯å€‹æ¨¡å‹ ---
    for model_idx, target_llm in enumerate(llm_name_range):
        print(f"\nğŸš€ [{model_idx+1}/{len(llm_name_range)}] Starting Experiments for Model: {target_llm}")
        
        # åŠ å…¥ try-exceptï¼Œç¢ºä¿å–®ä¸€æ¨¡å‹å¤±æ•—ä¸å½±éŸ¿å…¶ä»–æ¨¡å‹
        try:
            # --- å…§å±¤è¿´åœˆï¼šéæ­·æ¯ç¨®æƒ…ç·’ ---
            # ä½¿ç”¨ tqdm é¡¯ç¤ºè©²æ¨¡å‹çš„é€²åº¦
            for emotion_name, has_emotion_flag in tqdm(experiments, desc=f"Testing {target_llm.split('.')[1] if '.' in target_llm else target_llm}"):
                
                # 1. è¤‡è£½ä¸¦è¨­å®š Agent 1
                current_agent1 = agent1_basic_config.copy()
                current_agent1["llm_name"] = target_llm
                current_agent1["has_emotion"] = has_emotion_flag
                current_agent1["emotion"] = emotion_name if has_emotion_flag else ""
                
                # 2. æº–å‚™éŠæˆ²è¨­å®š
                final_game_config = prepare_game_description(
                    config=game_basic_config, 
                    naming_config=naming_config
                )
                
                # 3. æº–å‚™ Agent 1 (è®€å– Prompt)
                final_agent1_config = prepare_agent_config(
                    config=current_agent1,
                    game_name=final_game_config["name"],
                    naming_config=naming_config,
                    agent_ind=1,
                )
                
                # ======================================================
                # [é—œéµä¿®æ­£] å¼·åˆ¶å°‡ emotion å­—ä¸²å¯«å›è¨­å®šæª”ï¼Œä»¥å…è¢« prepare å‡½å¼å¼„ä¸Ÿ
                # ======================================================
                final_agent1_config["emotion"] = emotion_name if has_emotion_flag else ""
                # ======================================================
                
                # 4. æº–å‚™ Agent 2
                final_agent2_config = prepare_agent_config(
                    config=agent2_basic_config,
                    game_name=final_game_config["name"],
                    naming_config=naming_config,
                    agent_ind=2,
                )
                
                # 5. åˆå§‹åŒ– Logger
                # å¾æ¨¡å‹åç¨±æå–ç°¡çŸ­ç‰ˆæœ¬ (ä¾‹å¦‚ "meta.llama3-8b-instruct-v1:0" -> "llama3-8b")
                model_short_name = target_llm.split('.')[-1].split('-instruct')[0].split('-v')[0]
                logger = TwoAgentsLogger.construct_from_configs(
                    final_agent1_config, 
                    final_agent2_config, 
                    LOG_PATH, 
                    game_name=final_game_config['name'],
                    model_suffix=model_short_name
                )
                
                # 6. åŸ·è¡ŒéŠæˆ²
                run_game(final_game_config, naming_config, final_agent1_config, final_agent2_config, logger)
                
                # 7. çµ±è¨ˆ
                evaluate_statistics = DecisionStatistics(logger.run_name, LOG_PATH)
                decision_stats, count_combinations = evaluate_statistics.get_metric()
                
                save_readable_config(
                    {"decision_stats": decision_stats, "count_combinations": count_combinations},
                    logger.run_name,
                    LOG_PATH,
                )

        except Exception as e:
            print(f"\nâŒ Critical Error with model {target_llm}: {str(e)}")
            print("Skipping to next model...")
            # traceback.print_exc() # å¦‚æœæƒ³çœ‹è©³ç´°éŒ¯èª¤è¨Šæ¯å¯æ‰“é–‹é€™è¡Œ

    print("\n" + "="*60)
    print("âœ… All experiments finished!")
